\subsubsection{Support Vector Machines}

\paragraph{}
	Here, we present the theory for Support Vector Machines (SVMs) based on the lecture notes from Prof. Andrew Ng \cite{ng13}. SVMs are one of the most widely used and many argue among the best "off-the-shelf" supervised learning algorithms. This is mainly due to the sound theoretical framework, efficiency and good generalisation guarantees even for high-dimensional and linearly non-separable data.
	
\paragraph{Notation}
	Having $m$ training examples, where
	
	\begin{itemize}

  		\item $\vec{x}^{(i)} \in \mathbb{R}^d$ is the $d$-dimensional $i$-th training data.
  		\item $y^{(i)} \in \left\{-1, 1 \right\}$ is the $i$-th training label.

	\end{itemize}
	
	we want to find the parameters $\vec{w} \in \mathbb{R}^d$ which describe the hyperplane $\vec{w}^T \vec{x} + b = 0$ that separates our two classes. Thus, we can define our classifier as $h_{\vec{w}, b}(\vec{x}) = g\left(\vec{w}^T \vec{x}\right)$, such that $g(z) = 1$ if $z \geq 0$ and $g(z) = -1$ otherwise. Note that this is a non-probabilistic learning model as we are not considering the probability of each class or the data.
	
\paragraph{Objectives}
	The main intuition behind SVMs is that 