\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Design Process}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Questionnaire selection}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Signal Analysis -- simple method}{11}}
\citation{wiki:machineLearning}
\citation{ng13}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Machine Learning -- Theory}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Support Vector Machines}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Notation}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Objectives}{13}}
\newlabel{eqn:svm}{{5.1}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Illustration of the optimal margin classifier.\relax }}{14}}
\newlabel{fig:svm}{{5.1}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Lagrange duality}{14}}
\newlabel{eqn:lagrangian}{{5.2}{14}}
\newlabel{eqn:dual1}{{5.3}{14}}
\newlabel{eqn:dual2}{{5.4}{14}}
\newlabel{eqn:dualForm}{{5.5}{15}}
\newlabel{eqn:dualOpt}{{5.6}{15}}
\newlabel{eqn:bStar}{{5.7}{15}}
\newlabel{eqn:svmClassifier}{{5.8}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Kernel trick}{15}}
\citation{mlBook}
\citation{ghahramani2000variational}
\newlabel{eqn:polyKernel}{{5.9}{16}}
\newlabel{eqn:rbfKernel}{{5.10}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Soft margin SVMs}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Our problem}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}State-Space Models}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Probabilistic Graphical Model of a SSM and a HMM\relax }}{17}}
\newlabel{fig:pgm}{{5.2}{17}}
\citation{wiki:HMM}
\citation{rabiner1989tutorial}
\citation{ramage07}
\citation{mlBook}
\@writefile{toc}{\contentsline {subsubsection}{Our problem}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Hidden Markov Models}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Discrete observations}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{19}}
\@writefile{toc}{\contentsline {paragraph}{Solution to the first problem.}{19}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {Forward Procedure} for computing $\alpha _t(i)$.\relax }}{19}}
\newlabel{alg:forward}{{1}{19}}
\@writefile{toc}{\contentsline {paragraph}{Solution to the second problem.}{19}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textsc  {Viterbi Algorithm} for computing the most likely sequence of hidden states.\relax }}{20}}
\newlabel{alg:viterbi}{{2}{20}}
\@writefile{toc}{\contentsline {paragraph}{Solution to the third problem.}{20}}
\citation{wiki:EM}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \textsc  {Backward Algorithm} for computing $\beta _t(i)$.\relax }}{21}}
\newlabel{alg:backward}{{3}{21}}
\newlabel{eqn:hmmGamma}{{5.19}{21}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces \textsc  {Forward-Backward Algorithm (Baum-Welch)} for estimating HMM parameters $\lambda $.\relax }}{21}}
\newlabel{alg:baumwelch}{{4}{21}}
\@writefile{toc}{\contentsline {paragraph}{Parameters estimation with known hidden states.}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Continuous observations}{22}}
\@writefile{toc}{\contentsline {paragraph}{}{22}}
\newlabel{eqn:gaussian}{{5.23}{22}}
\@writefile{toc}{\contentsline {paragraph}{Changes to the algorithms.}{22}}
\citation{mlBook}
\@writefile{toc}{\contentsline {paragraph}{Different probability density functions.}{23}}
\@writefile{toc}{\co